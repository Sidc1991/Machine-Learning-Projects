#LIBRARY
library(dplyr)
library(data.table)
#instal.packages('quantmod')
library(quantmod)
library(ggplot2)
library(scales)
#to drop 1st autocorrelation lag only
library(TSA)
#for ljung box
library(LSTS)
#Time series library for augmented dickey-fuller
library(tseries)
#Library for ARMA
library(forecast)
#To calculate RMSE
library(Metrics)
#time related changes
library(lubridate)
#==============================
#IMPORT DATASET
al.returns <- read.csv("/Users/siddharth/Desktop/Dissertation/Data/eikon/al_shfe_prices.csv")
View(al.returns)
#=============================
#PLOT CLOSING PRICE ALUMINIUM
al.returns$Date <- as.Date(al.returns$Date)
require(ggplot2)
ggplot(data = al.returns, aes(Date, CLOSE)) + ylab("Aluminium Closing Price") + geom_line(color = "#00AFBB") + theme_minimal() + scale_x_date(breaks = date_breaks("years"), labels = date_format("%y"))

#stat_smooth(color = "#FC4E07", fill = "#FC4E07", method = "loess")

#=============================
#Select variables of interest
alm <- subset(al.returns, select = c(Date, CLOSE))
#=============================
#Variable Transform To Returns
alm$returns <- Delt(alm$CLOSE)
#=============================
#Select New Variables
alm <- subset(alm, select = c(Date, returns))
#=============================
#Drop NAs
alm <- alm[-c(1),]
plot.ts(alm$returns)
#============================
#Rename dataset for ease of later
lag <- alm
#==============================================
#CREATE NEW DATASET MERGING WITH EXOGENOUS FEATURES
#==============================================
#1) IMPORT AUDUSD
AUDUSD <- read.csv("~/Desktop/Dissertation/Baseline Model/Data/AUDUSD.csv")

#Rename Column
colnames(AUDUSD) <- c("X.DateTime.", "AUDUSD")

#Merge datasets
lag_merge <- merge(transform(lag, Date = format(as.Date(Date), "%Y-%m-%d")), transform(AUDUSD, Date = format(as.Date(X.DateTime.), "%Y-%m-%d")))

#Drop redundant date time column
lag_merge <- subset(lag_merge, select = -c(X.DateTime.))

#-----------------------------------------------------

#2) IMPORT CLPUSD
CLPUSD <- read.csv("~/Desktop/Dissertation/Baseline Model/Data/CLPUSD.csv")

#Rename Column
colnames(CLPUSD) <- c("X.DateTime.", "CLPUSD")

#Merge datasets
lag_merge <- merge(transform(lag_merge, Date = format(as.Date(Date), "%Y-%m-%d")), transform(CLPUSD, Date = format(as.Date(X.DateTime.), "%Y-%m-%d")))

#Drop redundant date time column
lag_merge <- subset(lag_merge, select = -c(X.DateTime.))

#------------------------------------------------
#3) IMPORT 3 MONTH TREASURY BILL RATES
fed_treasury3 <- read.csv("~/Desktop/Dissertation/Baseline Model/Data/FRED-DTB3.csv")
View(fed_treasury3)

#Rename column
colnames(fed_treasury3) <- c("Date", "Treasury")

#Merge datasets
lag_merge <- merge(transform(lag_merge, Date = format(as.Date(Date), "%Y-%m-%d")), transform(fed_treasury3, Date = format(as.Date(Date), "%Y-%m-%d")))
#-------------------------------------------------
#4) IMPORT TED SPREAD
ted.spread <- read.csv("~/Desktop/Dissertation/Baseline Model/Data/FRED-TEDRATE.csv")
View(ted.spread)

#Rename column
colnames(ted.spread) <- c("Date", "Ted")
#Merge datasets
lag_merge <- merge(transform(lag_merge, Date = format(as.Date(Date), "%Y-%m-%d")), transform(ted.spread, Date = format(as.Date(Date), "%Y-%m-%d")))

#-------------------------------------------------
#5) IMPORT GOLDMAN SACHS COMMODITY INDEX
GSCI <- read.csv("~/Desktop/Dissertation/Baseline Model/Data/GSCI.csv")
View(GSCI)
#Rename column
colnames(GSCI) <- c("Date", "GSCI")
#Merge datasets
lag_merge <- merge(transform(lag_merge, Date = format(as.Date(Date), "%Y-%m-%d")), transform(GSCI, Date = format(as.Date(Date), "%Y-%m-%d")))
#-----------------------------------
#6) IMPORT VOLATILITY INDEX (Losing data here)
CHOEM_VIX_Mine <- read.csv("~/Desktop/Dissertation/Baseline Model/Data/CHOEM_VIX_Mine.csv")
View(CHOEM_VIX_Mine)
#Select only Prev..Day.Open.Interest, Close_VIX, Date
VIX <- subset(CHOEM_VIX_Mine, select = c(Trade.Date, Prev..Day.Open.Interest, Close_VIX) )
#Rename column
#colnames(VIX) <- c("Date", "Prev..Day.Open.Interest","Close_VIX")
VIX$Date <- parse_date_time(VIX$Trade.Date, 'mdy')
#Merge datasets
lag_merge <- merge(transform(lag_merge, Date = format(as.Date(Date), "%Y-%m-%d")), transform(VIX, Date = format(as.Date(Date), "%Y-%m-%d")))
#Drop redundant information
lag_merge <- subset(lag_merge, select = -c(Trade.Date))

#-----------------------------------
#7) IMPORT BALTIC DRY INDEX (Ignore losing too much data)
#BDI <- read.csv("~/Desktop/Dissertation/Baseline #Model/Data/LLOYDS-BDI.csv")
#View(BDI)

#------------------------------------
#8) WTI
WTI <- read.csv("~/Desktop/Dissertation/Baseline Model/Data/WTI.csv")
View(WTI)
#Rename column
colnames(WTI) <- c("Date", "WTI")
#Merge datasets
lag_merge <- merge(transform(lag_merge, Date = format(as.Date(Date), "%Y-%m-%d")), transform(WTI, Date = format(as.Date(Date), "%Y-%m-%d")))

#======================================

#======================================
# CREATE TIME SERIES EXPERTS
#======================================
#Libraries
library(opera)
library(mgcv)
library(caret)
library(RColorBrewer)

#------------------------
# LOAD DATA
#------------------------
attach(lag_merge)

#-------------------------
#GENERALIZED ADDITIVE MODELS
#--------------------------
gam.mix <- gam(returns ~ s(AUDUSD) + s(CLPUSD) +s(Treasury) + s(Ted) + s(GSCI) + s(Prev..Day.Open.Interest ) + s(Close_VIX) + s(WTI), data = lag_merge)

gam.pred <- predict(gam.mix, newdata = lag_merge)
#-------------------------
#Gradient Boosting Machine
#-------------------------
gbm.mix <- train(returns ~ AUDUSD + CLPUSD + Treasury + Ted + GSCI + Prev..Day.Open.Interest + Close_VIX + WTI, data = lag_merge, method = 'gbm')

gbm.pred <- predict(gbm.mix, newdata = lag_merge)
#================================
#Building Forecaster
Y <- lag_merge$returns
X <- cbind(gam.pred, gbm.pred)
matplot(cbind(Y,X), type = 'l', col = 1:6, ylab = "Returns", xlab = "Time", main = "Expert forecast and observation")

#=================================
#Plot the oracle
oracle.convex <- oracle(Y = Y, experts = X, loss.type = "square", model = "convex")
plot(oracle.convex)

#---------------------------------------
# Online Gradient Descent 
#---------------------------------------
OGD <- mixture(Y = Y, experts = X, model = "OGD", loss.type = 'square')
summary(OGD)
plot(OGD, pause = TRUE, col = brewer.pal(9,name = "Set1"))

#-----------------------------------
# Exponentially Weighted Averages (EWAF)
#------------------------------------

EWA <- mixture(Y = Y, experts = X, model = "EWA", loss.type = 'square')
summary(EWA)
plot(EWA, pause = TRUE, col = brewer.pal(9,name = "Set1"))
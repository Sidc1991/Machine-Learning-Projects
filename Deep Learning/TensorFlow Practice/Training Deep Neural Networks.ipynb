{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28 #MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING DEEP NEURAL NETWORKS\n",
    "\n",
    "Three issues with training deep neural networks:\n",
    "\n",
    "1. Vanishing or exploding gradient problem.\n",
    "2. Training network is slow.\n",
    "3. Risk of overfitting with millions of parameters.\n",
    "\n",
    "## 1) Vanishing Gradient Problem\n",
    "\n",
    "### Choice of weight initialization\n",
    "\n",
    "Xavier or He initialization of weights (based on activation function) instead of $\\sigma(0,1)$ initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable h1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/siddharth/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 208, in variable\n    caching_device=caching_device)\n  File \"/Users/siddharth/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/siddharth/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 244, in model_variable\n    caching_device=caching_device, device=device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-57adfe5cf152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhe_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_scaling_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m hidden1 = fully_connected(X, n_hidden1, weights_initializer = he_init, \n\u001b[0;32m----> 4\u001b[0;31m                           scope = \"h1\")\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mfully_connected\u001b[0;34m(inputs, num_outputs, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[1;32m   1344\u001b[0m                                        \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                                        \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_collections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                                        trainable=trainable)\n\u001b[0m\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# Reshape inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mmodel_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device)\u001b[0m\n\u001b[1;32m    242\u001b[0m                   \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                   \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                   caching_device=caching_device, device=device)\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device)\u001b[0m\n\u001b[1;32m    206\u001b[0m                                        \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                                        \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                                        caching_device=caching_device)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m   1022\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    848\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    344\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    329\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    630\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 632\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    633\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable h1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/siddharth/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 208, in variable\n    caching_device=caching_device)\n  File \"/Users/siddharth/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/siddharth/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 244, in model_variable\n    caching_device=caching_device, device=device)\n"
     ]
    }
   ],
   "source": [
    "#For some reason this cell throws an error if you run it twice.\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = fully_connected(X, n_hidden1, weights_initializer = he_init, \n",
    "                          scope = \"h1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "\n",
    "Poor activation function can lead to vanishing gradient. ReLU did not suffer from this issue. However, ReLU suffered from dying units problem where, if the input to a neuron become negative, the output would become 0 and the neural unit would be unable to learn any further. \n",
    "\n",
    "Leaky ReLU solver this issue where negative inputs would not lead to dead units but rather units in a coma that can be revived.\n",
    "\n",
    "$$ LeakyRelu_{\\alpha}(z) = Max(\\alpha z,z)  $$\n",
    "\n",
    "Where $\\alpha$ determines what value the negative relu should converge towards (usually between 0.01 and 0.2).\n",
    "\n",
    "An improvement on leaky relu which made the curve more smoother at 0 was the Exponential Linear Unit (ELU). This overcame the dying units problem as well as made convergence faster.\n",
    "\n",
    "$$ELU_{\\alpha}(z) = \\alpha(exp(z) - 1)$$ if $z < 0$\n",
    "$$ELU_{\\alpha}(z) = z$$ if  $z >= 0$\n",
    "\n",
    "Code for ELU is following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = fully_connected(X, n_hidden1, activation_fn = tf.nn.elu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for leaky relu needs to be defined :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name = None):\n",
    "    return tf.maximum(0.01*z, z, name = name)\n",
    "\n",
    "hidden1 = fully_connected(X, n_hidden1, activation_fn = leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Weight initialization and proper activation prevent vanishing gradients at the beginning of training but not during. In this case we need to use batch normalization.\n",
    "\n",
    "Each layer in a neural net has a simple goal, to model the input from the layer below it, so each layer tries to adapt to it’s input but for hidden layers, things get a bit complicated. The input’s statistical distribution changes after a few iterations, so if the input statistical distribution keeps changing, called internal covariate shift, the hidden layers will keep trying to adapt to that new distribution hence slowing down convergence. It is like a goal that keeps changing for hidden layers.\n",
    "\n",
    "So the batch normalization (BN) algorithm tries to normalize the inputs to each hidden layer so that their distribution is fairly constant as training proceeds. This improves convergence of the neural net.\n",
    "\n",
    "BN acts like a regularizer reducing the need for other regularization techniques like dropout. \n",
    "\n",
    "We will use batch_norm() command (and not batch_normalization() command) to implement our BN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool, shape = (), name = 'is_training')\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None  \n",
    "}\n",
    "\n",
    "hidden1 = fully_connected(X, n_hidden1, scope = \"hidden1\", \n",
    "                          normalizer_fn = batch_norm, \n",
    "                          normalizer_params=bn_params)\n",
    "\n",
    "hidden2 = fully_connected(hidden1, n_hidden2, scope = \"hidden2\",\n",
    "                         normalizer_fn = batch_norm, normalizer_params = bn_params)\n",
    "\n",
    "logits = fully_connected(hidden2, n_outputs, activation_fn = None,\n",
    "                        scope = \"outputs\", normalizer_fn=batch_norm,\n",
    "                        normalizer_params = bn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of code:\n",
    "\n",
    "- is_training will either be True or False and determines whether batch_norm() should use the current mini-batch's mean and standard deviation or the running averages that it keeps track of during test set.\n",
    "\n",
    "- bn_params is a dictionary that will be passed to batch_norm function. The algorithm uses exponential decay to caculate to compute the running averages, which is why it requires decay parameters.\n",
    "\n",
    "- Given new value v, the running average v' is updated through:\n",
    "\n",
    "$$v' <- v'(decay) + v(1-decay)$$\n",
    "\n",
    "- A good decay value is close to 1 (0.9, 0.99 etc). \n",
    "- Updates collection set to None to automatically update running averages.\n",
    "- Lastly we create the layers by calling fully_connected() function.\n",
    "\n",
    "To avoid repeating the same parameters over and over again you can make an *argument scope* using the arg_scope() function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to put reuse = TRUE because weight for these variables is already\n",
    "#defined above.\n",
    "with tf.contrib.framework.arg_scope(\n",
    "    [fully_connected], normalizer_fn = batch_norm,\n",
    "    normalizer_params = bn_params):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope = \"hidden1\", reuse=True)\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope = \"hidden2\",reuse=True)\n",
    "    logits = fully_connected(hidden2, n_outputs, scope = \"outputs\", \n",
    "                           activation_fn = None, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution phase is also prett much the same, with one exception. Whenever you run an operation that depends on the batch_norm layer, you need to set the is_training placeholder to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "#    sess.run(init)\n",
    "#    for epoch in range(n_epochs):\n",
    "#        for X_batch, y_batch in zip(x_batches, y_batch):\n",
    "#            sess.run(training_op, feed_dict = {is_training:True, X:X_batch, \n",
    "#                                          y: y_batch})\n",
    "#            accuracy_score = accuracy.eval(\n",
    "#                feed_dict = {is_training:False, X:X_test_scaled, y:y_test})\n",
    "#            print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "Preventing the gradient exploding problem instead of vanishing gradient. Clip the gradients during backpropogation so that they never exceed some threshold. \n",
    "\n",
    "We will achieve this using the *clip_by_value()* function in compute_gradients() method and apply it using apply_gradients() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold = 1.0\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#grads_and_cars = optimizer.compute_gradients(loss)\n",
    "#capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "#             for grad, var in grad_and_vars]\n",
    "#training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "\n",
    "It is generally not a good idea to re-train very large DNN from scratch. Instead, find an existing NN that accomplishes a similar task and reuse the lower layers. This is called transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "#os.chdir('/Users/siddharth/Desktop/TensorFlow/tmp2/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[....]construct original model\n",
    "\n",
    "#If we wanted to restore the original model we would use the following:\n",
    "#with tf.Session() as sess:\n",
    "#    saver.restore(sess, \"my_original_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in general we will only reuse part of the original model. To configure Saver to restore only a subset of the variables from the original model. For example, the following code restores only hidden layers 1,2 and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ellipsis]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[...]\n",
    "##build new model with the same definition as before for hidden layers 1-3\n",
    "\n",
    "#init = tf.global_variables_initializer()\n",
    "\n",
    "#reuse_vars = tf.get_collection(tf.GraphKeys, TRAINABLE_VARIABLES,\n",
    "#                              scope = \"hidden[123]\")\n",
    "\n",
    "#reuse_vars_dict = dict([(var.name, var.name) for var in reuse_vars])\n",
    "#original_saver = tf.Saver(reuse_vars_dict) #saver to restore the original model\n",
    "\n",
    "#new_saver = tf.Saver() #saver to save new model\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    sess.run(init)\n",
    "#    original_saver.restore(\"./my_original_model.ckpt\")#restore layer 1 to 3\n",
    "#    [...] #train the new model\n",
    "#    new_saver.save(\"./my_new_model.ckpt\")#save the whole model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we build the new model, making sure to copy the original model's hidden layers 1 to 3.\n",
    "\n",
    "- We also create node to initialize all variables and keep only the ones whose scope matches the expression \"hidden[123]\" (i.e. we get all the trainable variables in hidden layers 1 to 3).\n",
    "\n",
    "- Then we get a list of all variables created.\n",
    "\n",
    "- Next we create a dictionary mapping the name of each variable in the original model to its name in the new model (generally you want to keep the exact same names).\n",
    "\n",
    "- Then we create a Saver that will restore only these variables, and we create another Saver to save the entire new model, not just layers 1 to 3.\n",
    "\n",
    "- We then start a session and initialize all variables in the model, then restore the variable values from original model's layers 1 to 3. \n",
    "\n",
    "- Finally we train the model on the new task and save it.\n",
    "\n",
    "### Freezing the Lower Layers\n",
    "\n",
    "It is likely that the lower layers of DNN have learned to detect low level features in pictures that will be useful across both image classification tasks, so you can just reuse these layers as they are. it is generally a good idea to \"freeze\" their weights when training the new DNN: if the lower-layer weight are fixed, then the higher-layer weights are easier to train.\n",
    "\n",
    "To freeze lower layers during training, the simplest solution is to give the optimizer the list of variables to train, excluding the variables from the lower layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "#                              scope = \"hidden[34]outputs\")\n",
    "#training_op = optimizer.minimize(loss, var_list = train_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line gets a list of trainable variables in hidden layers 3 and 4 as well as the output layer. This leaves out the variables in hidden layers 1 and 2.\n",
    "\n",
    "The second line ensures only hidden layers specified in the first line (3 and 4) are optimized thus freezing layer 1 and 2. \n",
    "\n",
    "### Caching the Frozen Layers\n",
    "\n",
    "Since the frozen layers won't change, it is possible to cache the output of the topmost frozen layer for each training instance. Since training goes through the whole dataset many times, this will give you a huge speed bosost as you will only need to go through the frozen layers once per training instance (instead of once per epoch). For example, you could first run the whole training set through the lower layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden2_outputs = sess.run(hidden2, feed_dict = {X: X_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then during training, instead of building batches of training instances, you would build batches of outputs from hidden layer 2 and feed them to the training operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "n_batches = 500\n",
    "\n",
    "#for each in range(n_epochs):\n",
    "#    shuffle_idx = rnd.permutation(len(hidden2_outputs))\n",
    "#    hidden2_batches = np.array_split(hidden2_outputs[shuffled_idx], n_batches)\n",
    "#    y_batches = np.array.split(y_train[shuffle_idx, n_batches])\n",
    "#    for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "#        sess.run(training_op, feed_dict={hidden2: hidden2_batch, y:y_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line runs the training operation defined earlier and feeds it a batch of outputs from second hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Training Network is Slow\n",
    "\n",
    "### Faster Optimizers\n",
    "\n",
    "- Momentum Optimization\n",
    "- Nesterov Accelerated Gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam Optimization\n",
    "\n",
    "(Just use Adam)\n",
    "\n",
    "Code for using any of these optimizers is simple. For example: the momentum optimizer can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, \n",
    "                                       momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "\n",
    "Setting learning rate high and then reducing it over time to reach the optimal learning rate. Common types of learning rate schedule are:\n",
    "\n",
    "- Predetermined piecewise constant learning rate\n",
    "- Performance scheduling\n",
    "- Exponential scheduling\n",
    "- Power scheduling\n",
    "\n",
    "Of these exponential decay is favored where we set the learning rate to a function of the iteration number $t:\\eta(t) = \\eta_{0}10^{-t/r}$. It requires tuning $\\eta_{0}$ and *r*. The learning rate will drop by a factor of 10 every r steps.\n",
    "\n",
    "The implementation code is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "decay_steps = 10000\n",
    "decay_rate = 1/10\n",
    "global_step = tf.Variable(0, trainable = False)\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                        global_step, decay_steps,\n",
    "                                        decay_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum = 0.9)\n",
    "training_op = optimizer.minimize(loss, global_step = global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting the hyperparameter values, we create a non-trainable variable global_step (initialized to 0) to keep track of the current training iteration number. \n",
    "\n",
    "Then we define the exponentially decaying learning rate (with $\\eta_{0} = 0.1$ and $r = 10,000$ using TensorFlows exponential_decay() function.\n",
    "\n",
    "Next, we create an optimizer using this decaying learning rate (momentum in above case)..\n",
    "\n",
    "Finally, create training operation calling optimizer's minimize() method.\n",
    "\n",
    "Adaptive optimizers automatically reduce learning rate during training, it is not necessary to add an extra learning schedule. Use ofr other optimization algos.\n",
    "\n",
    "## 3) Avoiding Overfitting Through Regularization\n",
    "\n",
    "- Early stopping\n",
    "- L1 regularization\n",
    "- L2 regularization\n",
    "- Dropout\n",
    "- Max-norm regularization\n",
    "- Data augmentation\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "One way to implement this in TF is to evaluate the model on a validation set at regular intervals (eg. every 50 steps), and save a \"winner\" snapshot if it outperforms previous \"winner\" snapshots. Count the number of steps since the last \"winner\" snapshot was saved, and interrupt training when this number reaches some limit (eg 2000 steps). Then restore the last \"winner\" snapshot.\n",
    "\n",
    "Although early stopping works well in practice, you can usually get much higher performance out of your network by combining it with regularization techniques.\n",
    "\n",
    "### L1 and L2 Regularization\n",
    "\n",
    "Constrain neural network's connection weights (but typically not its biases). \n",
    "\n",
    "One way to do this using TF is to simply add the regularization term to your cost function. For example, assuming you have just one hidden layer with weights weights1 and one output layer with weights weights2, then you can apply L1 regularization like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ellipsis]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[...]#construct they neural network\n",
    "#base_loss = tf.reduce_mean(xentropy, name = \"avg_xentropy\")\n",
    "#reg_losses = tf.reduce_sum(tf.abs(weights1) + tf.reduce_sum(tf.abs(weights2)))\n",
    "#loss = tf.add(base_loss, scale*reg_losses, name = \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more convenient way to implement L1/L2 regularizer is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arg_scope = tf.contrib.framework.arg_scope\n",
    "#with arg_scope(\n",
    "#    [fully_connected],\n",
    "#    weights_regularizer = tf.contrib.layers.l1_regularizer(scale = 0.1)):\n",
    "#    hidden1 = fully_connected(X, n_hidden1, scope = \"hidden1\")\n",
    "#    hidden2 = fully_connected(hidden1, n_hidden2, scope = \"hidden2\")\n",
    "#    logits = fully_connected(hidden2, n_outputs, activation_fn=None, \n",
    "#                            scope = \"out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a NN with two hidden layers and output layer, and it also creates nodes in the graph to compute L1 regularization loss corresponding to each layer's weights.\n",
    "\n",
    "Tensorflow automatically adds these nodes to a special collection containing all the regularization losses. You just need to add these regularization losses to your overall loss, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([base_loss] + reg_losses, name = \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "The most popular regularization technique for deep neural networks is arguably dropout.\n",
    "\n",
    "At every training step, every neuron has a probability p of being temporarily \"dropped out\" meaning it will be entirely ignored during this training step, but it may be active during the next step.\n",
    "\n",
    "Dropout rate is typically set to 50%.\n",
    "\n",
    "To implement dropout using TensorFlow we can simply apply the dropout() function to the input layer and to the output of every hidden layer. During training, this function randomly drops some neurons and divides the remaining neurons by the keep probability. After training this function does nothing at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ellipsis]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[...]\n",
    "\n",
    "#is_training = tf.placeholder(tf.bool, shape = (), name = \"is_training\")\n",
    "\n",
    "#keep_prob = 0.5\n",
    "#X_drop = dropout(X, keep_prob, is_training = is_training)\n",
    "\n",
    "#hidden1 = fully_connected(X_drop, n_hidden1, scope = \"hidden1\")\n",
    "#hidden1_drop = dropout(hidden1, keep_prob, is_training = is_training)\n",
    "\n",
    "#hidden2 = fully_connected(hidden1, drop, n_hidden2, scope = \"hidden2\")\n",
    "#hidden2_drop = dropout(hidden2, keep_prob, is_training = is_training)\n",
    "\n",
    "#logits = fully_connected(hidden2_Dropout, n_outputs, activation_fn = None,\n",
    "#                        scope = \"outputs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, just like you did earlier for batch normalization, you need to set is_training to True when training and False when testing.\n",
    "\n",
    "### Max-Norm Regularization\n",
    "\n",
    "Max-norm regularization ensures that for each neuron, it constrains the weights w of the incoming connections such that IIwII <= r where r is the max-norm hyperparameter. \n",
    "\n",
    "TensorFlow does not provide off the shelf max_norm regularizer. The following code creates a node clip_weights that will clip the weights of the variable along with the second axis so that each row vector has a maximum norm of 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold = 1.0\n",
    "#clipped_weights = tf.clip_by_norm(weights, clip_norm = threshold, \n",
    "#                                  axes = 1)\n",
    "#clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would then apply this operation after each training step, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "#    [...]\n",
    "#    for epoch in range(n_epochs):\n",
    "#        [...]\n",
    "#        for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "#            sess.run(training_op, feed_dict = {X: x_batch, y: y_batch})\n",
    "#            clip_weights.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?zip\n",
    "\n",
    "#The zip() function take iterables (can be zero or more), \n",
    "#makes iterator that aggregates elements based on the iterables \n",
    "#passed, and returns an iterator of tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder how to get access to the weights variable of each layer. For this you can simply use a variable scope like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"hidden1/weights/read:0\", shape=(784, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#hidden1 = fully_connected(X, n_hidden1, scope = \"hidden1\")\n",
    "\n",
    "with tf.variable_scope(\"hidden1\", reuse = True):\n",
    "    weights1 = tf.get_variable(\"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print out all the global variables we can use the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1/weights:0\n",
      "h1/biases:0\n",
      "fully_connected/weights:0\n",
      "fully_connected/biases:0\n",
      "fully_connected_1/weights:0\n",
      "fully_connected_1/biases:0\n",
      "hidden1/weights:0\n",
      "hidden1/BatchNorm/beta:0\n",
      "hidden1/BatchNorm/moving_mean:0\n",
      "hidden1/BatchNorm/moving_variance:0\n",
      "hidden2/weights:0\n",
      "hidden2/BatchNorm/beta:0\n",
      "hidden2/BatchNorm/moving_mean:0\n",
      "hidden2/BatchNorm/moving_variance:0\n",
      "outputs/weights:0\n",
      "outputs/BatchNorm/beta:0\n",
      "outputs/BatchNorm/moving_mean:0\n",
      "outputs/BatchNorm/moving_variance:0\n",
      "Variable:0\n",
      "hidden1/weights/Momentum:0\n",
      "hidden1/BatchNorm/beta/Momentum:0\n",
      "hidden2/weights/Momentum:0\n",
      "hidden2/BatchNorm/beta/Momentum:0\n",
      "outputs/weights/Momentum:0\n",
      "outputs/BatchNorm/beta/Momentum:0\n"
     ]
    }
   ],
   "source": [
    "#To print out all the global variables we can use the following command\n",
    "\n",
    "for variable in tf.global_variables():\n",
    "    print(variable.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the preceeding solution should work fine, it is a bit messy. A cleaner solution is to create a max_norm_regularizer() function and use it just like the earlier l1_regularizer() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes = 1, name = \"max_norm\",\n",
    "                        collection = \"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm = threshold, axes = axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name = name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None #there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns parameterized max_norm() function that you can use like any other regularizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_norm_reg = max_norm_regularizer(threshold = 1.0)\n",
    "#hidden1 = fully_connected(X, n_hidden1, scope = \"hidden1\",\n",
    "#                         weights_regularizer = max_norm_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to fetch these clipping operations and run them after each training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    [...]\n",
    "#    for epoch in range(n_epochs):\n",
    "#        [...]\n",
    "#        for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "#            sess.run(training_op, feed_dict = {X:X_batch, y:y_batch})\n",
    "#            sess.run(clip_all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation \n",
    "\n",
    "Stops overfitting therefore regularization technique.\n",
    "\n",
    "TensorFlow offers several image manipulation operations such as transposing (shifting), rotating, resizing, flipping, and cropping, as well as adjusting the brightness, contrastm saturation and hue making it easy to implement data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

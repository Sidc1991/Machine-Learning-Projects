#Preprocessing

data("USArrests")
df <- scale(USArrests)
head(df, nrow = 6)

#======

#SIMILARITY MEASURES

# In R software, you can use the function dist() to compute the distance between every pair of object in a data set. The results of this computation is known as a distance or dissimilarity matrix.

#By default, the function dist() computes the Euclidean distance between objects; however, it’s possible to indicate other metrics using the argument method. See ?dist for more information.
#For example, consider the R base data set USArrests, you can compute the distance matrix as follow:

#Compute dissimilarity matrix
#df = the standardized data

res.dist <- dist(df, method = "euclidean")

#To see easily the distance information between objects, we reformat the results of the function dist() into a matrix using the as.matrix() function. Element 1,2 represent the distance between objects 1 and object 2, etc.

as.matrix(res.dist)[1:6, 1:6]

#======

#LINKAGE

#The linkage function takes the distance information, returned by the function dist(),
#and groups pairs of objects into clusters based on their similarity. Next, these newlyformed clusters are linked to each other to create bigger clusters. This process isiterated until all the objects in the original data set are linked together in a #hierarchical tree.

#For example, given a distance matrix “res.dist” generated by the function dist(), the
#R base function hclust() can be used to create the hierarchical tree

res.hc <- hclust(d = res.dist, method = "ward.D2")

#• d: a dissimilarity structure as produced by the dist() function.
#• method: The agglomeration (linkage) method to be used for computing distance
#between clusters. Allowed values is one of “ward.D”, “ward.D2”, “single”,
#“complete”, “average”, “mcquitty”, “median” or “centroid”.

#======

#DENDROGRAM

#Dendrograms correspond to the graphical representation of the hierarchical tree
#generated by the function hclust(). Dendrogram can be produced in R using the
#base function plot(res.hc), where res.hc is the output of hclust(). Here, we’ll use the function fviz_dend()[ in factoextra R package].

library("factoextra")
fviz_dend(res.hc, cex = 0.5)

#The height of the fusion, provided on the vertical axis, indicates the (dis)similarity/distance between two objects/clusters. The higher the height of the fusion, the less similar the objects are. This height is the cophenetic distance between two objects.

#Note that, conclusions about the proximity of two objects can be drawn only based on
#the height where branches containing those two objects first are fused. We cannot use
#the proximity of two objects along the horizontal axis as a criteria of their similarity.

#========

#VERIFY THE CLUSTER TREE

# One way to measure how well the cluster tree generated by the hclust() function reflects your data is to compute the correlation between the cophenetic distances and the original distance data generated by the dist() function.

#If the clustering is valid, the linking of objects in the cluster tree should have a strong correlation with the distances between objects in the original distance matrix.

#The closer the value of the correlation coefficient is to 1, the more accurately the
#clustering solution reflects your data. Values above 0.75 are felt to be good. The
#“average” linkage method appears to produce high values of this statistic. This may
#be one reason that it is so popular.

#The R base function cophenetic() can be used to compute the cophenetic distances for
#hierarchical clustering.

# **********
#Compute cophenetic distance
res.coph <- cophenetic(res.hc)

# Correlation between cophenetic distance and the original (euclidean) distance

cor(res.dist, res.coph)

#Execute the hclust() function again using the average linkage method. Next, call cophenetic() to evaluate the clustering solution.

res.hc2 <- hclust(res.dist, method = "average")
cor(res.dist, cophenetic(res.hc2))

# *********

#CUT THE DENDROGRAM INTO DIFFERENT GROUPS

#You can cut the hierarchical tree at a given height in order to partition your data
#into clusters. The R base function cutree() can be used to cut a tree, generated by
#the hclust() function, into several groups either by specifying the desired number of
#groups or the cut height. It returns a vector containing the cluster number of each
#observation.

#Cut tree into 4 groups
grp <- cutree(res.hc, k = 4)
head(grp, n = 4)

#Number of memebers in each cluster
table(grp)

#Get the number for the members of cluster 1
rownames(df)[grp == 1]

#===========

#VISUALIZING THE CLUSTERS

#The result of the cuts can be visualized easily using the function fviz_dend() [in factoextra]:
# Cut in 4 groups and color by groups

fviz_dend(res.hc, k = 4, # Cut in four groups
cex = 0.5, # label size
k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
color_labels_by_k = TRUE, # color labels by groups
rect = TRUE # Add rectangle around groups
)


#Using the function fviz_cluster() [in factoextra], we can also visualize the result in a scatter plot. Observations are represented by points in the plot, using principal components. A frame is drawn around each cluster.

fviz_cluster(list(data = df, cluster = grp),
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "convex", # Concentration ellipse
repel = TRUE, # Avoid label overplotting (slow)
show.clust.cent = FALSE, ggtheme = theme_minimal())

#=====================
#=====================

# CLUSTER R PACKAGE (SAVES A LOT OF TIME)

#The R package cluster makes it easy to perform cluster analysis in R. It provides
#the function agnes() and diana() for computing agglomerative and divisive clustering,
#respectively. These functions perform all the necessary steps for you. You don’t need
#to execute the scale(), dist() and hclust() function separately.

library("cluster")
# Agglomerative Nesting (Hierarchical Clustering)
res.agnes <- agnes(x = USArrests, # data matrix
stand = TRUE, # Standardize the data
metric = "euclidean", # metric for distance matrix
method = "ward" # Linkage method
)

#==============

#After running agnes() and diana(), you can use the function fviz_dend()[in factoextra] to visualize the output:

fviz_dend(res.agnes, cex = 0.6, k = 4)

#================================
#---------PART 2----------------
#================================

# COMPARING DENDROGRAMS**
# VERY USEFUL FOR COMPARING DIFFERENT DENDROGRAM METRICS AND METHODS

#The dendextend package provides several functions for comparing dendrograms. Here,
#we’ll focus on two functions:
#• tanglegram() for visual comparison of two dendrograms
#• and cor.dendlist() for computing a correlation matrix between dendrograms

set.seed(123)
#subset containing 10 rows
ss <- sample(1:50, 10)
df <- df[ss,]

#We start by creating a list of two dendrograms by computing hierarchical clustering (HC) using two different linkage methods (“average” and “ward.D2”). Next, we transform the results as dendrograms and create a list to hold the two dendrograms.

library(dendextend)

#Step 1:
# Compute distance matrix
res.dist <- dist(df, method = "euclidean")

#Step 2:
# Compute 2 hierarchical clusterings
hc1 <- hclust(res.dist, method = "average")
hc2 <- hclust(res.dist, method = "ward.D2")

#Step 3:
#Create two dendrograms
dend1 <- as.dendrogram(hc1)
dend2 <- as.dendrogram(hc2)

#Step 4:
#Create list to hold dendrograms
dend_list <- dendlist(dend1, dend2)

#===============

#VISUAL COMPARISON OF TWO DENDROGRAMS

#To visually compare two dendrograms, we’ll use the tanglegram() function [dendextend package], which plots the two dendrograms, side by side, with their labels connected by lines.
#The quality of the alignment of the two trees can be measured using the function entanglement(). Entanglement is a measure between 1 (full entanglement) and 0 (no entanglement). A lower entanglement coefficient corresponds to a good alignment.

tanglegram(dend1, dend2,
highlight_distinct_edges = FALSE, # Turn-off dashed lines
common_subtrees_color_lines = FALSE, # Turn-off line colors
common_subtrees_color_branches = TRUE, # Color common branches
main = paste("entanglement =", round(entanglement(dend_list), 2))
)

#======

#CORRELATION MATRIX BETWEEN A LIST OF DENDROGRAMS

# The function cor.dendlist() is used to  “Cophenetic” correlation matrix between a list of trees. The value can range between -1 to 1. With near 0 values meaning that the two trees are not statistically similar.

#Cophenetic correlation matrix
cor.dendlist(dend_list, method = "cophenetic")

#Baker correlation matrix
cor.dendlist(dend_list, method = "baker")

#The correlation between two trees can also be computed as follows:
cor_cophenetic(dend1, dend2)

#Baker correlation coefficient
cor_bakers_gamma(dend1,dend2)

#============
#VISUALIZE MULTIPLE DENDROGRAM TECHNIQUES SIMULTANEOUSLY

#It’s also possible to compare simultaneously multiple dendrograms. A chaining operator %>% is used to run multiple function at the same time. It’s useful for simplifying the code:

#Create multiple dendrograms by chaining
dend1 <- df %>% dist %>% hclust("complete") %>% as.dendrogram
dend2 <- df %>% dist %>% hclust("single") %>% as.dendrogram
dend3 <- df %>% dist %>% hclust("average") %>% as.dendrogram
dend4 <- df %>% dist %>% hclust("centroid") %>% as.dendrogram

#Compute Correlation Matrix

dend_list <- dendlist("Complete" = dend1, "Single" = dend2, "Average" = dend3, "Centroid" = dend4)

cors <- cor.dendlist(dend_list)

#Print correlation matrix
round(cors, 2)

# Visualize the correlation matrix using corrplot package
#install.packages("corrplot")
library(corrplot)
corrplot(cors, "pie", "lower")

#==============
#==============

# VISUALIZING DENDROGRAMS***

#We show how to save and to zoom a large dendrogram.
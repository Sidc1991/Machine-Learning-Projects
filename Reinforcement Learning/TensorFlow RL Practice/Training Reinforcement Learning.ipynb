{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI Gym\n",
    "\n",
    "One of the challenges of RL is that in order to train an agent you first need to have a working environment. OpenAI gym is a toolkit that provides a wide variety of simulated environments so you can train agents, compare them, or develop new RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 2.5MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /Users/siddharth/anaconda3/lib/python3.7/site-packages (from gym) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.10.4 in /Users/siddharth/anaconda3/lib/python3.7/site-packages (from gym) (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.0 in /Users/siddharth/anaconda3/lib/python3.7/site-packages (from gym) (2.19.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/siddharth/anaconda3/lib/python3.7/site-packages (from gym) (1.11.0)\n",
      "Collecting pyglet>=1.2.0 (from gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 1.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/siddharth/anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.24,>=1.21.1 in /Users/siddharth/anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/siddharth/anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym) (2018.8.24)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in /Users/siddharth/anaconda3/lib/python3.7/site-packages (from requests>=2.0->gym) (2.7)\n",
      "Collecting future (from pyglet>=1.2.0->gym)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz (829kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 1.4MB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
      "  Running setup.py bdist_wheel for gym ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/siddharth/Library/Caches/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/siddharth/Library/Caches/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
      "Successfully built gym future\n",
      "Installing collected packages: future, pyglet, gym\n",
      "Successfully installed future-0.17.1 gym-0.10.9 pyglet-1.3.2\n"
     ]
    }
   ],
   "source": [
    "#Installing OpenAI Gym\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install --upgrade gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the first environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04303083,  0.04961465,  0.00299637, -0.02529808])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "obs = env.reset()\n",
    "obs\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The make() function creates an environment, in this case a CartPole environment. This is a 2D simulation in which a cart can be accelerated left or right in order to balance a pole placed on top of it.\n",
    "\n",
    "After the environment is created, we must initialize the reset() method. this returns the first observation. Observation depends on the type of environment. For the CartPole environment, each observation is a 1D NumPy array containing four floats: these floats represent the cart's horizontal position (0.0 = center), its velocity, the angle of the pole (0.0 = vertical) and its angular velocity. \n",
    "\n",
    "Finally, the render() method displays the environment.\n",
    "\n",
    "If you want the render() to return the rendered image as a NumPy array, you can set the mode parameter to rgb_array(note that other environments may support different modes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render(mode = \"rgb_array\")\n",
    "img.shape #height, width, channels (3 = RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let,s ask the environment which actions are possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete(2) means that the possible actions are integers 0 and 1, which represent accelerating left(0) and right (1). Other environments may have more discrete kinds of actions. \n",
    "\n",
    "Since pole is leaning towards the right, lets accelerate it towards the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04203854,  0.2446935 ,  0.00249041, -0.31703411])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 1 #accelerate right\n",
    "obs, reward, done, info = env.step(action)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step() method executes the given action and returns four values:\n",
    "\n",
    "*obs*\n",
    "\n",
    "This is the new observation. The cart is now moving towards the right (obs[1] > 0). The pole is still tilted towards the right (obs[2]>0), but its angular velocity is now negative (obs[3]<0), so it will likely be tilted towards the left after the next step.\n",
    "\n",
    "*reward*\n",
    "\n",
    "In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep running as long as possible.\n",
    "\n",
    "*done*\n",
    "\n",
    "This value will be True when the episode is over. This will happen when the pole tilts too much. After that, the environment must be reset before it can be used again.\n",
    "\n",
    "*info*\n",
    "\n",
    "This dictionary may provide extra debug information in other environments. This data should NOT be used be for training (it would be cheating).\n",
    "\n",
    "Let's hardcode a simple policy that accelerates left when pole is leaning toward the left and accelerates right when pole is leaning towards the right. We will run this policy to see the average rewards it gets over 500 episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle <0 else 1\n",
    "\n",
    "totals = []\n",
    "\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000): #1000 steps max\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.738, 8.475928031785074, 24.0, 71.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with 500 tries, this policy never managed to keep the pole upright for more than 68 consecutive steps. Let's see if neural network can come up with a better policy.\n",
    "\n",
    "## Neural Network Policies\n",
    "\n",
    "The neural network will take an observation as input, and it will build a probability for each output action. It will then select an action randomly according to these estimated probabilities.\n",
    "\n",
    "The reason we pick action randomly from the estimated probabilities instead of the action WITH *highest* probability is because we want to **explore** the action space rather than just exploit it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Specify the neural network architecture\n",
    "\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4 #simple task, dont need more hidden neurons\n",
    "n_outputs = 1 #only 1 outputs the probability of accelerating left\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Build the neural network\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs])\n",
    "hidden = fully_connected(X, n_hidden, activation_fn = tf.nn.elu,\n",
    "                        weights_initializer = initializer)\n",
    "logits = fully_connected(hidden, n_outputs, activation_fn = None,\n",
    "                        weights_initializer = initializer)\n",
    "outputs = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Select a random action based on the estimated probabilities\n",
    "#no axis input exists in tf.concat. It's concat_dim now.\n",
    "p_left_and_right = tf.concat(concat_dim = 1, values = [outputs, \n",
    "                                                1-outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples = 1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cdoe:\n",
    "- After the imports, we define a neural network architecture.\n",
    "\n",
    "\n",
    "- Next we build the neural network. In this example, its a vanilla Multi-Layer Perceptron with a single output. Note that the output layer uses the logistic(sigmoid) activation function in order to output a probability from 0.0 to 1.0. If there were more outputs then we would use softmax activation instead.\n",
    "\n",
    "\n",
    "- Lastly, we call the multinomial() function to pick a random action. The function independently samples one(or more) integers, given the log probability of each integer, For example, if you call it with the array [np.log(0.5), np.log(0.2), np.log(0.3)] and with num_samples = 5, then it will output five integers, each of which will have a 50% probability of being 0, 20% of being 1 and 30% of being 2. \n",
    "\n",
    "\n",
    "- In our case we just need one integer representing the action to take. Since the outputs tensor only contains the probability of going left, we must first concatenate 1-outputs to it to have tensor containing the probability of both left and right actions. Note that if there were more than two possible actions, the neural network would have to output one probability per action so you would not need a concatenation step.\n",
    "\n",
    "## Evaluating Actions: The Credit Assignment Problem\n",
    "\n",
    "Since there are many actions that can be taken before seeing an agent fail, it becomes difficult to determine which particular action/s were responsible for this failure. In these situations we apply a discounted weight (0.95) on each action taken from the present day to the past to put a weight on each of the actions taken. The rewards of which are normalized (subract mean, divide by std deviation) and the positive scoring results are considered.\n",
    "\n",
    "## Policy Gradients\n",
    "\n",
    "Creating gradients for an agent in a system over multiple trials. The positive gradients are reinforced by moving in the direction of those gradients and the negative gradients are reinforced by moving away from the direction of these gradients. The magnitude of movement depends on the reward scores of each action.\n",
    "\n",
    "The implementation is as follows. \n",
    "\n",
    "We will train the neural network policy we built earlier so that it learns to balance the pole on the cart. Let's start by completing the construction phase we coded earlier to add the target probability, cost function, and the training operation. Since we are acting as though the chosen action is the best possible action, the target probability must be 1.0 if chosen action is action 0(left) and 0.0 if it is action 1(right):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1. - tf.to_float(action)\n",
    "\n",
    "#Now we have target probability. We will use cross entropy to compute \n",
    "#gradients\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    targets = y, logits = logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we are calling the optimizer's compute_gradients() method instead of the minimize() method. This is because we want to tweak the gradients before we apply them.**\n",
    "\n",
    "The compute_gradients() method returns a list of gradient vector/variable pairs(one pair per trainable variable). Let's all put the gradients in a list, to make it more convenient to obtain their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "#print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the execution phase, the algorithm will run the policy and at each step it will evaluate these gradient tensors and store their values. \n",
    "\n",
    "After multiple iterations these gradients will be tweaked. \n",
    "\n",
    "Next it will need to feed the resulting gradients back to the optimizer so it can perform an optimization step.\n",
    "\n",
    "This means we need **one placeholder per gradeint vector.** \n",
    "\n",
    "Moreover, we must create the operation that will apply the updated gradients. For this we call the optimizer's apply_gradients() function, which takes a list of gradient vector/variable pairs.\n",
    "\n",
    "Instead of giving the original vectors, we will give it a list containing the updated gradients (i.e., the ones fed through gradient placeholders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape = grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "    \n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the execution phase. we will ned a couple of functions to compute the total discounted rewards given the raw rewards and to normalize result across multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards*discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards) \n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    rewards_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - rewards_mean)/rewards_std\n",
    "           for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22., -40., -50.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10,0,-50], discount_rate = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some error in the book here\n",
    "#discount_and_normalize_rewards([[10,0,-50], [10,20]],\n",
    "#                               discount_rate = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all we need to train the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 250      #number of training iterations\n",
    "n_max_steps = 1000      #max steps per episode\n",
    "n_games_per_update = 10 #train policy every 10 episodes\n",
    "save_iterations = 10    #Save the model every 10 training iterations\n",
    "discount_rate = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iterations in range(n_iterations):\n",
    "        all_rewards = [] #all sequences of raw rewards for each episode\n",
    "        all_gradients = [] #gradients saved at each step of each episode\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = [] #all raw rewards from the current episode\n",
    "            current_gradients = [] #all gradients from the current episode\n",
    "            obs = env.reset()\n",
    "            for steps in range(n_max_steps):\n",
    "                action_val, gradient_val = sess.run(\n",
    "                    [action, gradients],\n",
    "                    feed_dict = {X: obs.reshape(1, n_inputs)}) #one obs\n",
    "                obs, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        #At this point we have run the policy for 10 episodes, and we are\n",
    "        #ready for a policy update using alogirthm described earlier\n",
    "        \n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards)\n",
    "        feed_dict{}\n",
    "        \n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            #multiply the gradients by the action scores, and compute mean\n",
    "            mean_gradients = np.mean(\n",
    "            [rewards*all_gradients[game_index][step][var_index]\n",
    "            \n",
    "            for game_index, rewards in enumerate(allrewards)\n",
    "            for step, reward in enumerate(rewards)], axis = 0)\n",
    "            \n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict = feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "            saver.save(sess, \"./my_policy_net_pg.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each training iteration starts by running the policy for 10 episodes with maximum 1000 steps per episode. At each step we also compute the gradients pretending that the chosen action was the best. \n",
    " \n",
    " After these 10 episodes have been run, we compute the action scores using the discount_and_normalize_rewards() function;\n",
    " \n",
    " We go through each trainable variable, across all episodes and steps, to multiply each gradient vector by its corresponding action scorel and we compute the mean of the resulting gradients.\n",
    " \n",
    " Finally, we run the training operation, feeding it these mean gradients (one per trainable variable). We also save the model every 10 training oeprations\n",
    " \n",
    " \n",
    " # Markov Decision Processes\n",
    " \n",
    " Used in situation where there are multiple states and multiple actions in each step the agent moves through the system. To find the best state-action pairs we use something called the Q-values.\n",
    " \n",
    " We start by initializing all the Q-Value estimate to zero, then update them using the Q-value iteration algorithm. The optimal Q-Value of the state action pair (s,a) is the sum of discounted future rewards of the agent can expect on average after it reaches the state s and chooses action a, but before it sees the outcome of this action, assuming it behaves optimally after that. \n",
    " \n",
    "Let's apply this algorithm. First we need to define the Markov decision process (MDP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan #represents impossible actions\n",
    "\n",
    "T = np.array([#shape = [s, a, s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8,0.2,0.0]],\n",
    "    [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0,0.0,1.0]],\n",
    "    [[nan,nan,nan], [40,0.0,0.0], [nan,nan,nan]], \n",
    "])\n",
    "\n",
    "R = np.array([ #shape = [s,a,s']\n",
    "    [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "    [[10., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],\n",
    "    [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],\n",
    "])\n",
    "\n",
    "possible_actions = [[0,1,2], [0,2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's run the Q-value iteration algorithm:\n",
    "\n",
    "Q = np.full((3,3), -np.inf) #-inf for all impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0 #Initial value = 0.0 for all possible actions\n",
    "    \n",
    "learning_rate = 0.01\n",
    "discount_rate = 0.95\n",
    "n_iterations = 100\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s,a] = np.sum([\n",
    "                T[s,a,sp]*(R[s,a,sp] + discount_rate*np.max(Q_prev[sp]))\n",
    "                for sp in range(3)\n",
    "            ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.59419690e+39, 2.19540396e+39, 4.46126592e+39],\n",
       "       [1.35247138e+40,           -inf, 3.44628523e+40],\n",
       "       [          -inf, 8.78161582e+40,           -inf]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q, axis = 1) #optimal action for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an optimal policy for MDP, when using a discount rate of 0.95: in state $s_0$ choose action $a_0$, in state $s_1$ choose $a_2$ (go through fire), and in state $s_2$ choose actions $a_1$ (the only possible action)\n",
    "\n",
    "Interestingly, if you reduce discount rate to 0.9, the optimal polic changes: in state $s_1$ the best action becomes $a_0$ (stay put, dont go through fire). It makes sense because if you value present more than future, then the prospect of future rewards is not worth immediate pain.\n",
    "\n",
    "## Temporal Difference and Q-Learning\n",
    "\n",
    "Using the TD Learning and Q-Learning algorithms we try and take into account the fact that only the different states and actions are available to an agent. The transition probabilities and rewards are not. In such a scenario we introduce the TD Learning and Q-learning algorithm which work similarly to stochastic gradient descent and learn over time the state-reward pairs through an update mechanism. \n",
    "\n",
    "It can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000\n",
    "\n",
    "s = 0 #start in state 0\n",
    "\n",
    "Q = np.full((3,3), -np.inf) #-inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0 #initial value = 0.0, for all possible actions\n",
    "    \n",
    "    \n",
    "for iteration in range(n_iterations):\n",
    "    a = rnd.choice(possible_actions[s]) #choose an action randomly\n",
    "    sp = rnd.choice(range(3), p = T[s,a]) #pick next state using T[s,a]\n",
    "    reward = R[s, a, sp]\n",
    "    learning_rate = learning_rate0/(1+iteration * learning_rate_decay)\n",
    "    Q[s, a] = learning_rate * Q[s,a] + (1 - learning_rate) * (\n",
    "        reward + discount_rate * np.max(Q[sp])\n",
    "    )\n",
    "    \n",
    "    s = sp #move to next state\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Play Ms. PAC-Man Using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install cmake needed to install atari evironments\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install cmake boost boost-python sdl2 swig wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install boost-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape #[height, width, channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are nine discrete actions available which correspond to the nine possible positions of the joystick (left, right, up, down, center, upper left, and so on), and the observations are screenshots of Atari screen, represented as 3D NumPy arrays.\n",
    "\n",
    "These images are a bit large, so we will create a small preprocessing function that will crop the image and shrink it down to 88x80 pixels, convert it to grascale, and improve the contrast of Ms.Pac-Man. This will reduce the amount f computations required by the DQN, and speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mspacman_color = np.array([210,164,74]).mean()\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] #crop and downsize\n",
    "    img = img.mean(axis = 2) #to greyscale\n",
    "    img[img==mspacman_color] = 0 #improve contrast\n",
    "    img = (img - 128)/128 - 1 #normalize from -1. to 1.\n",
    "    return img.reshape(88,80,1)\n",
    "\n",
    "img = preprocess_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original observation (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed observation (88×80 greyscale)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the DQN. It could just take state-action pair(s,a) as input, and output an estimate of the corresponding Q-value Q(s,a) but since the actions are discrete it is more convenient to use neural networks that takes only state s as input and outputs one Q-Value estimate per action.\n",
    "\n",
    "The DQN will be composed of three convolutional layers, followed by two fully connected layers, including the output layer. \n",
    "\n",
    "As we will see, the training algorithm we will use requires two DQNs, with the same architecture (but different parameters): one will be used to drive Ms. Pac-Man during training (the actor), and the other will watch the actor and learn from its trials and errors (the critic). At regular intervals we will copy the critic to the actor.\n",
    "\n",
    "Since we need two identical DQNs, we will create a q_network() function to build them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import convolution2d, fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4,2,1]\n",
    "conv_paddings = [\"SAME\"]*3\n",
    "conv_activation = [tf.nn.relu]*3\n",
    "n_hidden_in = 64*11*10 #conv3 has 64 maps of 11x10 each\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n #9 discrete actions are available\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_network(X_state, scope):\n",
    "    prev_layer = X_state\n",
    "    conv_layer = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        for n_maps, kernel_size, stride, padding, activation in zip(\n",
    "            conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "            conv_paddings, conv_activation):\n",
    "            prev_layer = convolution2d(\n",
    "                prev_layer, num_outputs = n_maps, kernel_size = kernel_size,\n",
    "                stride = stride, padding = padding, activation_fn = activation,\n",
    "                weights_initializer = initializer)\n",
    "            conv_layers.append(prev_layer)\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape = [-1, n_hidden_in])\n",
    "        hidden = fully_connected(\n",
    "            last_conv_layer_flat, n_hidden, activation_fn = hidden_activation,\n",
    "            weights_initializer = initializer)\n",
    "        outputs = fully_connected(\n",
    "            hidden, n_outputs, activation_fn = None,\n",
    "            weights_initializer = initializer)\n",
    "    \n",
    "    traininable_vars = tf.get_collection(tf.GraphKeys.TRAIANABLE_VARIABLES,\n",
    "                                        scope = scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var \n",
    "                             for var in trainable_vars}\n",
    "    \n",
    "    return outputs, trainable_vars_by_name\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the code defines the hyperparameters of the DQN architecture. The q_network() function creates the DQNm taking the envirionment's state X_State as input, and the name of the variable scope.\n",
    "\n",
    "Note that we will just use one observation to represent the environment's state since theres almost no hidden state (except for blinking objects and ghost's directions).\n",
    "\n",
    "The trainable_vars_by_name dictionary gathers all the trainable variables of this DQN.\n",
    "\n",
    "Now let's create the input placeholder, the two DQNs, and the operation to copy the critic DQN to the actor DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape = [None, input_height, input_width,\n",
    "                                             input_channels])\n",
    "\n",
    "actor_q_values, actor_vars = q_network(X_state, scope = \"q_networks/actor\")\n",
    "critic_q_values, critic_vars = q_network(X_state, scope = \"q_networks/critic\")\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name])\n",
    "           \n",
    "           for var_name, actor_var in actor_vars.items()]\n",
    "\n",
    "copy_critic_to_actor = tf.group(*copy_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

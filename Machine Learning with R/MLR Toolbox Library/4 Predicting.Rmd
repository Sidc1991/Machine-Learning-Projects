# PREDICTING OUTCOMES FOR NEW DATA

library(mlr)
#Predicting the target values for new observations is implemented the same way as most of the other predict methods in R. In general, all you need to do is call predict on the object returned by train and pass the data you want predictions for.

#There are two ways to pass the data:

#Either pass the Task via the task argument or pass a data.frame via the newdata argument.
#The first way is preferable if you want predictions for data already included in a Task.

#Just as train, the predict function has a subset argument, so you can set aside different portions of the data in Task for training and prediction 

#In the following example we fit a gradient boosting machine to every second observation of the BostonHousing data set and make predictions on the remaining data in bh.task.

#(Gradient boosting is generalization of adaboost where we figure out the weight on each weak learner with the help of gradient descent)

n = getTaskSize(bh.task)
train.set = seq(1, n, by = 2)
test.set = seq(2, n , by = 2)
lrn = makeLearner("regr.gbm", n.trees = 100)
mod = train(lrn, bh.task, subset = train.set)

task.pred = predict(mod, task = bh.task, subset = test.set)
task.pred

#The second way is useful if you want to predict data not included in the Task.
#Here we cluster the iris data without the target variable. All observations with an odd index are included in the Task and used for training. Predictions are made from remaining observations.

n = nrow(iris)
iris.train = iris[seq(1,n, by = 2), -5]
iris.test = iris[seq(2,n, by = 2), -5]
task = makeClusterTask(data = iris.train)
mod = train("cluster.kmeans", task)
newdata.pred = predict(mod, newdata = iris.test)
newdata.pred

#Note that for supervised learning you do not have to remove the target columns from the data. These columns are automatically removed prior to calling the underlying predict method of the learner.

#==================================

#ACCESSING THE PREDICTION

#Function predict returns a named list of class Prediction. Its most impoerant element is $data which is a data.frame that contains columns with the true values of the target variable (in case of supervised learning problems) and the predictions. Use 'as.data.frame' for direct access.

#In the following the predictions on the BostonHousing and the iris data sets are shown. As you may recall, the predictions in the first case were made from a Task and in the second case from a data.frame.

#Result of predict with data passed via task argument
head(as.data.frame(task.pred))

# Result of predict with data passed via newdata argument
head(as.data.frame(newdata.pred))

#Since our test set was every second variable, what response is showing is our classifiers predicted outcome compared to the truth which is the actual outcome.

#As you can see when predicting from a Task, the resulting data.frame contains an additional column, called id, which tells us which element in the original data set the prediction corresponds to.

#A direct way to access the true and predicted values of the target variable(s) is provided by functions getPredictionTruth and getPredictionResponse.

head(getPredictionTruth(task.pred))
head(getPredictionResponse(task.pred))

#=================================================

#REGRESSION: EXTRACTING STANDARD ERRORS:

#Some learners provide a standard erros for prediction which can be accessed in mlr. An overview is given by calling the function listLearners and setting properties = "se". By assigning FALSE to check.packages learners from packages which are not installed will be included in the overview.

listLearners("regr", check.packages = FALSE, properties = "se")[c("class", "name")]

#In this example we train a linear regression model o Boston Housing dataset. In order to calculate the standard errors set the predict.type to "se":

#Create a learner and specify predict.type
lrn.lm = makeLearner("regr.lm", predict.type = 'se')
mod.lm = train(lrn.lm, bh.task, subset = train.set)
task.pred.lm = predict(mod.lm, task = bh.task, subset = test.set)
task.pred.lm
mod.lm = train(lrn.lm, bh.task, subset = )

#The standard errors can be extracted using getPredictionSE

head(getPredictionSE(task.pred.lm))

#The predicted probabilities can be extracted from the Prediction using function getPredictionProbabilities. Here is another cluster analysis example. We use fuzzy c-means clustering on mtcars data set.

lrn = makeLearner("cluster.cmeans", predict.type = "prob")
mod = train(lrn, mtcars.task)

pred = predict(mod, task = mtcars.task)
head(getPredictionProbabilities(pred))

#For classification problems there are some more things worth mentioning. By default, class labels are predicted.

#Linear discriminant analysis on the iris data set.
mod = train("classif.lda", task = iris.task)
pred = predict(mod, task = iris.task)
pred

#In order to get the predicted posterior probabilities we have to create a Learner with the appropriate predict.type

lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)

pred = predict(mod, newdata = iris)
head(as.data.frame(pred))

#If you dont put as.data.frame you get other information as well.

#================

#CLASSIFICATION CONFUSION MATRIX:

#A confusion matrix can be obtained by calling calculateConfusionMatrix. The columns represent predicted and the rows true calsses of the labels.

calculateConfusionMatrix(pred)

#To get frequencies additional to the absolute numbers we can set relative = TRUE

conf.matrix = calculateConfusionMatrix(pred, relative = TRUE)
conf.matrix

#We can add absolute number of observations for each predicted and true class label to the matrix (both absolute and relative) by setting sums = TRUE.

calculateConfusionMatrix(pred, relative = TRUE, sums = TRUE)

#=====================

#CLASSIFICATION: ADJUSTING THE DECISION THRESHOLD

#We can set the threshold value that is used to map the predicted posterior probability to class labels. Note that for this purpose we need to create a Learner that predicts probabilities. For binary classification , the threshold determines when the positive class is predicted. The default is 0.5. Now, we set the threshold for the positive class to 0.9. To illustrate binary classification, we use the Sonar data set from the mlbench package.

lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = sonar.task)

#Label of the positive class
getTaskDesc(sonar.task)$positive

#Default threshold
pred1 = predict(mod, sonar.task)
pred1$threshold

#Set the threshold value for positive class
pred2 = setThreshold(pred1, 0.9)
pred2$threshold

#We can use pred1 and pred2 in a confusion matrix with these thresholds.

calculateConfusionMatrix(pred2)

#Note that in the binary case getPredictionProbabilities by default extracts the posterior probabilities of the positive class only

head(getPredictionProbabilities(pred1))

#But we can change that, too
head(getPredictionProbabilities(pred1, cl = c("M", "R")))

#===============

#VISUALIZING THE PREDICTION

#The function plotLearnerPrediction allows us to visualize predictions, eg for teaching purposes or exploring models. It trains the chosen learning method for 1 or 2 selected features and then displays the prediction with ggplot.

#For classification, we get a scatter plot of 2 features (by default the first 2 in the dataset).The type of symbol shows the true class labels of the data points. Symbols with white border indicate misclassified observations. The posterior probabilities (if the learner under consideration supports this) are represented by the background color where higher saturation means larger probabilities.

#The plot title displays the ID of the Learner (in the following example CART), its parameters, its training performance and its cross-validation performance. mmce stands for mean misclassification error, i.e., the error rate. See the sections on performance and resampling for further explanations.

lrn  = makeLearner("classif.rpart", id = "CART")
plotLearnerPrediction(lrn, task = iris.task)

#For clustering we also get a scatter plot of two selecte features. The color of the points indicates the predicted cluster.

lrn = makeLearner("cluster.kmeans")
plotLearnerPrediction(lrn, task = mtcars.task, features = c("disp", "drat"), cv = 0)

#For regression, there are two types of plots. The 1D plot shows the target values in relation to a single feature, the regression curve and, if the chosen learner supports this, the estimated standard error.

plotLearnerPrediction("regr.lm", features = "lstat", task = bh.task)

#The 2D variant as in the classification case, generates a scatter plot of 2 features. The fill color of the dors illustrates the value of the target variable "medv", the background colors show the estimated mean. The plot does not reprent the estimated standard error.

plotLearnerPrediction("regr.lm",features = c("lstat" ,"rm"), task = bh.task)